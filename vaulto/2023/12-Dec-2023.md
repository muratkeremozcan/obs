# December 2023

## Week of 2023-12-04

- [next-issue-tracker](https://github.com/muratkeremozcan/next-issue-tracker) @learning

- [types in lambdas -> OIpenAPI docs](https://github.com/muratkeremozcan/aws-cdk-in-practice/pull/19/files) @testing

- [Optic PoC in Refunds service - initial PR](https://github.com/helloextend/refunds-service/pull/394) @testing

## Week of 2023-12-11

- TestJS conf talk

- [authV3 in BEST](https://github.com/helloextend/backend-service-template/pull/890) @testing

- [AuthV3 in BEST locally](https://github.com/helloextend/backend-service-template/pull/892) @testing

- Local testing in CI [Best PR](https://github.com/helloextend/backend-service-template/pull/899), [GHA PR](https://github.com/helloextend/gha-reusable-workflows/pull/599) @testing

- [client test with optic:verify](https://github.com/helloextend/backend-service-template/pull/904) @testing ([optic capture PR](https://github.com/helloextend/backend-service-template/pull/835), [optic free](https://github.com/helloextend/backend-service-template/pull/829/files) [client-with-cy](https://github.com/helloextend/backend-service-template/pull/884/files))

- Platform engineering demo
<details><summary>Platform engineering demo details</summary>

## Problem 1: client testing is lacking

Our services publish clients to make it possible for consuming services to call the publisher’s api. These have many API versions, and package versions.

## Problem 2 : no way to test integration prior to deploying to the same environment

When/if there are changes to our schema, other services might find out too late.

## Problem 3: Manually having to update OpenAPI specs

When there is a new API version, teams have to copy the most recent, and manually add in the changes.

### Testing the published clients of our services

Clone and install [![img](https://github.githubassets.com/favicon.ico)https://github.com/helloextend/backend-service-template](https://github.com/helloextend/backend-service-template)

```bash
yarn start:local # starts the server
```

Execute the client tests

```bash
yarn test:client
```

At the time of writing, executes `./published-exports/client/v2.client.test.ts`.

> Some calls are mocked at the moment (local testing auth needs work), but they still verify the schema.

### Generating OpenAPI specs from types

At the time of writing, some of our services have OpenAPI docs (example: https://github.com/helloextend/refunds-service/blob/main/src/api-rest/versions/latest/specs/refunds/refunds-api.json )

The pain point is that updating them is a manual process; copy paste the latest, modify the json file with the changes.

We can automate this process:

- Export the response types from our lambdas
- Import them to a central location (example: [![img](https://github.githubassets.com/favicon.ico)https://github.com/helloextend/backend-service-template/tree/main/src/api-specs/v1](https://github.com/helloextend/backend-service-template/tree/main/src/api-specs/v1) )
- Use a script to generate `json.schema`s from the types
- Use the `json.schemas` to generate OpenAPI docs with the help of `openapi-types` library (example: https://github.com/helloextend/backend-service-template/blob/main/src/api-specs/v1/openapi.ts )

```bash
# resets the json files
# generates json.schemas
# generates openapi specs
yarn update:api-docs
```

### Schema governance with Optic

Once we have an OpenAPI spec (whether we already have them like some of our services, or we auto-generate them from types), Optic can:

- Verify if the specification is valid OpenAPI
- Track any changes and evaluate if they would be breaking

```bash
# akin to snapshot testing,
# compares OpenAPI schemas on main to the current ones on the PR
yarn optic:diff
```

### Verifying the OpenAPI spec, by running client tests

Optic has another capability where it can sniff on http tests we specify (in this case our fast-running client tests) and verify them against the OpenAPI spec. This gives us:

- A real oracle to verify our client tests against; the OpenAPI spec (are we doing real things?)
- API coverage: are our client tests actually covering what we claim to publish?

```bash
# STOP the local server
# (the script handles that, for the purpose of 1:1 CI execution vs local)
 yarn optic:verify
```

You can find the sample PR [![img](https://github.githubassets.com/favicon.ico)https://github.com/helloextend/backend-service-template/pull/904](https://github.com/helloextend/backend-service-template/pull/904) and use Best template to reproduce all the above with the repo [![img](https://github.githubassets.com/favicon.ico)https://github.com/helloextend/backend-service-template](https://github.com/helloextend/backend-service-template) .

</details>

## Week of 2023-12-18

- [Smerf learning](https://github.com/helloextend/weekly-logs/blob/main/smerf-notes.md) @learning
- [BEST with full examples from Smerf course](https://github.com/helloextend/backend-service-template/pull/911) @learning

Optic paid vs free

- PR comments.
- Centralized api style governance for all your services. (You don’t have to go to each repo and tinker with it)
- An internal catalogue of all your APIs + changes so developers always know how the latest version works. This is much easier to read than openapi.json files in the repos, and the changes between versions are visualized for easier comprehension.
- Support.
- With free version we have to run check against each version of our api : optic diff src/api-specs/v1/openapi.json --base main --check . With paid we can just optic diff-all --match **/**/openapi.json --check and that will run against all your api files (or we use a script)
